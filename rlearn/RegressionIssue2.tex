\batchmode
\makeatletter
\def\input@path{{E:/workspace/testsync/rlearn//}}
\makeatother
\documentclass[english]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{babel}
\usepackage{amsmath}
\usepackage[unicode=true,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{pdftitle={LyTeX xeCJK template},
 pdfauthor={zoho@ctex},
 pdfsubject={latex},
 unicode=false}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
% please do NOT remove these lines!
% set file encoding for WinEdt
% !Mode:: "TeX:UTF-8"
% set typesetting program for TeXworks
% !TEX program = XeLaTeX

\usepackage[CJKtextspaces,CJKnumber]{xeCJK}                                      
\setCJKmainfont[BoldFont=SimHei]{SimSun}
\setCJKsansfont{SimHei}         
\setCJKmonofont{NSimSun}  

\setCJKfamilyfont{song}{SimSun}
\setCJKfamilyfont{hei}{SimHei}

\makeatother
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}

\begin{document}

\title{稳健简单线性回归}


\author{蒋冰}

\maketitle

\section{问题介绍}

大多数传统的统计方法是在对研究的总体的分布形式和其他特征做出一些假设的情况下的，而实际总体不满足假设条件，或观察数据中包含不能代表总体特性的异常点（Outliers）时，传统的统计方法可能就会出现较大误差。由于仪器故障、操作失误等原因，异常点是很容易出现的。因此，偏离假设是常见的现象。通常，数据量比较少和自变量个数不多时，可由散点图或残差图等找出异常点，但当样本量增大或自变量个数增多时，检测出异常点就会变得很困难。而且，即使找出了异常点，若异常点不是由于记录、录入等人为误差造成的，剔除异常点就没有充分理由。所以需要一种估计方法能既不剔除由随机误差造成的客观存在的异常点，又不会对回归系数影响太大的回归方法，即具有稳健性。

估计的稳健性(Robustness)指的是在估计过程中产生的估计量对模型误差的不敏感性。因此稳健估计是在比较宽的资料范围内产生的优良估计。如在独立同分布正态误差的线性模型中，最小二乘估计(Ordinary
Least Square, OLS )是有效无偏估计。然而当误差是非正态分布时，OLS不一定是最有效的。但误差分布事先不一定知道，故有必要考虑稳健回归的问题。稳健回归(Robust
Regression)估计，如误差为正态时，它比OLS稍差一点，但误差非正态时，它比OLS要好得多。这种对误差项分布的稳健特性，常能有效排除异常值干扰。


\section{方法}


\subsection{基本思想}

稳健估计讨论问题的方式是:对于实际问题有一个假定模型，同时又认为这个模型并不准确，而只是实际问题理论模型的一个近似。它要求解决这类问题的估计方法应达到以下目标：
\begin{enumerate}
\item 假定的观测分布模型下，估值应是最优的或接近最优的。
\item 当假设的分布模型与实际的理论分布模型有较小差异时，估值受到粗差的影响较小。
\item 当假设的分布模型与实际的理论分布模型有较大偏离时，估值不至于受到破坏性影响。
\end{enumerate}
稳健估计的基本思想是：在粗差不可避免的情况下，选择适当的估计方法，使参数的估值尽可能避免粗差的影响，得到正常模式下的最佳估值。稳健估计的原则是要充分利用观测数据（或样本）中的有效信息，限制利用可用信息，排除有害信息。由于事先不大准确知道观测数据中有效信息和有害信息所占比例以及它们具体包含在哪些观测中，从抗差的主要目标着眼是要冒损失一些效率的风险，去获得较可靠的、具有实际意义的、较有效的估值。 


\subsection{M估计}

在最小二乘法估计中，回归系数$\hat{\beta}$是使${\displaystyle \sum_{i=1}^{n}e_{i}^{2}}$达到最小，其中${\displaystyle e_{i}=y_{i}-\sum_{j=1}^{m}\beta_{i}x_{ij}}_{i}$,若令$\rho(x)=x^{2}$,则$\hat{\beta}$使${\displaystyle \sum_{i=1}^{n}\rho(e_{i})=\sum_{i=1}^{n}e_{i}^{2}}$达到最小。显然函数$\rho(e_{i})=e_{i}^{2}$随|x|的增大而增大，为使上式达到最小，就必须照顾某些点，特别是一些异常点。因此最小二乘法估计会往往使得那些远离数据群体的数据(很可能是异常值)对残差平方和影响比其他数据大得多。这是因为最小二乘估计为了达到极小化残差平方和的目的，必须迁就远端的数据，所以异常值对于参数估计相当敏感。

M估计可以看作是对最小二乘估计的一种改善。它用一个函数$\rho(x)$来代替每个观测值与估计值的残差平方和$x^{2}$，此函数除了随|x|的增大速度比$x^{2}$慢外，其他性质与$x^{2}$类似。并且通过对${\displaystyle \sum_{i=1}^{n}\rho(e_{i})}$取极小而得到的。对于函数$\rho$和样本，M估计是使得目标函数${\textstyle {\displaystyle \sum_{i=1}^{n}\rho(e_{i})}}$达到最小的估计。这便是M估计的想法。

$\rho(x)$是$x$的函数，它随$x$值的增大而增大，通常取$\rho(x)=-lnf(x)$，其中$f(x)$是$x$的分布密度函数。$\rho(x)$对取极小就相当于对$x$的似然函数的对数${\displaystyle \sum_{i=1}^{n}lnf(x)}$取极大。故M统计值就成为最大似然估计。若$\rho(x)=x^{2}$，M估计量等价于最小二乘估计量。

为了消除测量单位对估计的影响，类似于传统的将数据标准化的做法，在M估计中用$z_{i}=\frac{x_{i}}{s}$来代替$_{i}$。这里s是待估尺度（参数）（scale）。尺度s的稳健估计是另一个重要的稳健统计学问题。因此此文中将不考虑标准化。

令$\rho$关于$\beta$的导数为：$\psi(z_{j})=\frac{\partial\rho(e_{i})}{\partial\beta_{j}}$，则对原式子求导有，${\textstyle {\displaystyle \sum_{i=1}^{n}\psi(x_{i})x_{ij}}}=0,j=1,2,\cdots,m$，解此联立方程就可以解出$\beta$的值。

因为此方程较为复杂，要直接借出来比较困难，所以可以采用迭代法来求解。

令$w_{i}=\frac{\psi(e_{i})}{e_{i}}$,则 原式可以转换为${\textstyle {\displaystyle \sum_{i=1}^{n}\psi(x_{i})x_{ij}}}=$${\displaystyle \sum_{i=1}^{n}e_{i}}w_{i}x_{ij}=$${\displaystyle \sum_{i=1}^{n}({\displaystyle y_{i}-\sum_{i=1}^{n}\beta_{i}x_{ij}})w_{i}x_{ij}=0}$，所以有${\textstyle {\displaystyle \sum_{i=1}^{n}{\displaystyle w_{i}x_{ij}y_{i}}=\sum_{i=1}^{n}\beta_{i}x_{ij}w_{i}x_{ij}}}$，可以看出这是一个加权的最小二乘法。

为了能与最小二乘法做比较，因而在M估计中常将最小二乘估计作为初值，即取$\beta^{(0)}=\hat{\beta}$。

设$\beta^{(l)}$是第$l$次迭代值，其迭代公式为：$\beta^{(l+1)}=\beta^{(l)}+\Delta\beta^{(l)},l=0,1,2,\text{·}\text{·}\text{·}$，这里\foreignlanguage{english}{$\Delta\beta^{(l)}$}是增量。为求$\Delta\beta^{(l)}$好处假设$\psi(x)$在$\beta^{(l)}$处可以做泰勒展开，并取下面的近似表达式：$\psi(e_{i})\approx\psi(e_{i}^{l})-{\displaystyle \sum_{j=1}^{m}\psi'(e_{i}^{l})x_{ij}\Delta\beta_{j}^{(l)}},i=1,2,3,\cdots,n$其中，${\displaystyle e_{i}^{(l)}=y_{i}-\sum_{j=1}^{m}\beta_{i}^{(l)}x_{ij}}_{i}$,将展开式代入方程组，可得:

${\textstyle {\displaystyle \sum_{i=1}^{n}\psi(e_{i})x_{ij}}}={\textstyle {\displaystyle \sum_{i=1}^{n}(\psi(e_{i}^{l})-{\displaystyle \sum_{j=1}^{m}\psi'(e_{i}^{l})x_{ij}\Delta\beta_{j}^{(l)}})x_{ij}}},j=1,2,\cdots,m$，上述方程租是关于$\Delta\beta_{1}^{(l)},\Delta\beta_{2}^{(l)},\cdots,\Delta\beta_{m}^{(l)}$的线性方程租。因而可以解出\foreignlanguage{english}{$\Delta\beta^{(l)}$}。为了让迭代不会无限循环，需要事前设定好收敛原则：事先确定一个$\varepsilon>0$，要求$\underset{j}{max}|\Delta\beta_{j}^{(l)}|<\varepsilon$，便停止迭代。

下面列出几种常用的$\rho$函数、$\psi$ 函数：
\begin{enumerate}
\item Huber函数\\
$\rho(x)=\begin{cases}
\frac{1}{2}x^{2} & |x|\leq k\\
k|x|-\frac{1}{2}x^{2} & |x|>k
\end{cases}$\\
$\psi(x)=\begin{cases}
x & |x|\leq k\\
0 & |x|>k
\end{cases}$\\
$w(x)=\begin{cases}
1 & |x|\leq k\\
\frac{k}{|x||} & |x|>k
\end{cases}$
\item Hampel三段截尾函数\\
$\rho(x)=\begin{cases}
\frac{1}{2}x^{2} & |x|\leq a\\
k|x|-\frac{1}{2}x^{2} & a<|x|\leq b\\
ab-\frac{1}{2}a^{2}+\left(c-b\right)\frac{a}{2}\left[1-\left(\frac{c-|x|}{c-b}\right)^{2}\right] & b<|x|\leq c\\
ab-\frac{1}{2}a^{2}+\left(c-b\right)\frac{a}{2} & |x|>c
\end{cases}$\\
$\psi(x)=\begin{cases}
x & |x|\leq a\\
a\text{·}sign(x) & a<|x|\leq b\\
a\text{·}\frac{c-|x|}{c-b}\text{·}sign(x) & b<|x|\leq c\\
0 & |x|>c
\end{cases}$\\
$w(x)=\begin{cases}
1 & |x|\leq a\\
\frac{a}{|x|} & a<|x|\leq b\\
a\text{·}\frac{c-|x|}{c-b}\text{·}sign(x) & b<|x|\leq c\\
0 & |x|>c
\end{cases}$
\item Tukey的双二次函数\\
$\rho(x)=\begin{cases}
\frac{1}{6}\left[1-\left(1-x^{2}\right)^{3}\right] & |x|\leq1\\
\frac{1}{6} & |x|>1
\end{cases}$\\
$\psi(x)=\begin{cases}
x\left(1-x^{2}\right)^{2} & |x|\leq1\\
0 & |x|>1
\end{cases}$\\
$w(x)=\begin{cases}
\left(1-x^{2}\right)^{2} & |x|\leq1\\
0 & |x|>1
\end{cases}$
\end{enumerate}

\section{数值模拟}

为了比较稳健估计和最小二乘法，本文随机生成了一些数据，然后分别用两种估计方法进行回归分析。

本文首先随机选了m =10个x值，然后用$y=3.5x+6+\varepsilon$来生成10个y值。再将最后的2个y值改成符合函数$y=3.5x+12+\varepsilon$的2个异常点。

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
n <- 10  \hlcom{# 样本量}
m <- 2  \hlcom{# 离群点数量,m<n}
r <- \hlkwd{rnorm}(n, 0, 1)  \hlcom{# 误差项}
x <- \hlkwd{runif}(n, -10, 50)  \hlcom{# 自变量x 取值在[a,b]}
beta1 <- 3.5
beta0 <- 6
betax <- 12
y <- beta1 * x + beta0 + r  \hlcom{#  y=b0+b1*x}
y[(n - m + 1):n] <- betax * x[(n - m + 1):n] + betax + r[(n - m + 1):n]
\end{alltt}
\end{kframe}
\end{knitrout}


然后进行回归分析。其中model\_1即为最小二乘法回归，model\_2为采用huber函数的稳健回归，model\_3为采用Hampel三段截尾函数的稳健回归，model\_4为采用Tukey的双二次函数的稳健回归。

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
model_1 <- \hlkwd{lm}(y ~ x)
\hlkwd{library}(\hlstr{"MASS"})
model_2 <- \hlkwd{rlm}(y ~ x, psi = psi.huber)
model_3 <- \hlkwd{rlm}(y ~ x, psi = psi.hampel)
model_4 <- \hlkwd{rlm}(y ~ x, psi = psi.bisquare)
\end{alltt}
\end{kframe}
\end{knitrout}


下图即为数据的散点图和回归直线。

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}(y ~ x)
\hlkwd{abline}(model_1, col = \hlstr{"black"}, lty = \hlstr{"solid"})
\hlkwd{abline}(model_2, col = \hlstr{"red"}, lty = \hlstr{"dotted"})
\hlkwd{abline}(model_3, col = \hlstr{"blue"}, lty = \hlstr{"dashed"})
\hlkwd{abline}(model_4, col = \hlstr{"green"}, lty = \hlstr{"dotdash"})
\hlkwd{legend}(\hlstr{"right"}, \hlkwd{c}(\hlstr{"OLS"}, \hlstr{"huber"}, \hlstr{"hampel"}, \hlstr{"bisquare"}), lty = \hlkwd{c}(\hlstr{"solid"}, \hlstr{"dotted"}, 
    \hlstr{"dashed"}, \hlstr{"dotdash"}), col = \hlkwd{c}(\hlstr{"black"}, \hlstr{"red"}, \hlstr{"blue"}, \hlstr{"green"}))
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-3} 

\end{knitrout}


从上图可以看出，由于异常点的出现，最小二乘法显然为照顾异常点，与预期有所偏离。而稳健回归却很好地削弱了异常点的影响。

model\_1最小二乘法得到的回归方程的系数和残差图：

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}(model_1)
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -75.28 -29.82 -15.17  -3.89 135.07 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept)    78.72      43.83    1.80     0.11
## x               2.01       1.38    1.46     0.18
## 
## Residual standard error: 64.9 on 8 degrees of freedom
## Multiple R-squared: 0.211,	Adjusted R-squared: 0.112 
## F-statistic: 2.14 on 1 and 8 DF,  p-value: 0.182
\end{verbatim}
\begin{alltt}
\hlkwd{plot}(\hlkwd{residuals}(model_1))
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-4} 

\end{knitrout}


model\_2为采用huber函数的稳健回归得到的回归方程的系数和残差图：

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}(model_2)
\end{alltt}
\begin{verbatim}
## 
## Call: rlm(formula = y ~ x, psi = psi.huber)
## Residuals:
##      Min       1Q   Median       3Q      Max 
##  -3.8106  -0.5529  -0.0887   0.7188 175.5865 
## 
## Coefficients:
##             Value  Std. Error t value
## (Intercept) 10.572  1.498      7.059 
## x            3.384  0.047     71.997 
## 
## Residual standard error: 1.14 on 8 degrees of freedom
\end{verbatim}
\begin{alltt}
\hlkwd{plot}(\hlkwd{residuals}(model_2))
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-5} 

\end{knitrout}


model\_3为采用Hampel三段截尾函数的稳健回归得到的回归方程的系数和残差图:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}(model_3)
\end{alltt}
\begin{verbatim}
## 
## Call: rlm(formula = y ~ x, psi = psi.hampel)
## Residuals:
##      Min       1Q   Median       3Q      Max 
##  -1.0754  -0.6476   0.0636   1.6577 176.9751 
## 
## Coefficients:
##             Value   Std. Error t value
## (Intercept)   7.926   0.895      8.852
## x             3.447   0.028    122.631
## 
## Residual standard error: 1.55 on 8 degrees of freedom
\end{verbatim}
\begin{alltt}
\hlkwd{plot}(\hlkwd{residuals}(model_3))
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-6} 

\end{knitrout}


model\_4为采用Tukey的双二次函数的稳健回归得到的回归方程的系数和残差图:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}(model_4)
\end{alltt}
\begin{verbatim}
## 
## Call: rlm(formula = y ~ x, psi = psi.bisquare)
## Residuals:
##      Min       1Q   Median       3Q      Max 
##  -1.0445  -0.6221   0.0902   1.6892 177.0090 
## 
## Coefficients:
##             Value   Std. Error t value
## (Intercept)   7.885   0.938      8.405
## x             3.447   0.029    117.057
## 
## Residual standard error: 1.5 on 8 degrees of freedom
\end{verbatim}
\begin{alltt}
\hlkwd{plot}(\hlkwd{residuals}(model_4))
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-7} 

\end{knitrout}



\section{实际数据}

本文在实际数据中比较稳健估计和最小二乘法，在此选择的数据是Forbes数据。

在19世纪四五十年代，苏格兰物理学家James D.Forbes试图通过水的沸点来估计海拔高度。他在阿尔卑斯山及苏格兰手机数据。R语言的MASS包选取了他1857年的论文中的17个数据，本文选用的就是这个数据。

因为原数据中没有离群点，所以在数据中人为添加了2个异常点。

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}(MASS)
F <- forbes$bp
h <- forbes$pres
F <- \hlkwd{c}(F, 190)
h <- \hlkwd{c}(h, 10)
F <- \hlkwd{c}(F, 200)
h <- \hlkwd{c}(h, 11)
myforbes <- \hlkwd{data.frame}(F = F, h = h, logh = \hlkwd{log}(h), log100h = 100 * \hlkwd{log}(h))
\end{alltt}
\end{kframe}
\end{knitrout}


然后进行回归分析。其中model\_1即为最小二乘法回归，model\_2为采用huber函数的稳健回归，model\_3为采用Hampel三段截尾函数的稳健回归，model\_2为采用Tukey的双二次函数的稳健回归。

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}(log100h ~ F, data = myforbes)
\hlkwd{abline}(model_1, col = \hlstr{"black"})
\hlkwd{abline}(model_2, col = \hlstr{"red"}, lty = \hlstr{"dotted"})
\hlkwd{abline}(model_3, col = \hlstr{"blue"}, lty = \hlstr{"dashed"})
\hlkwd{abline}(model_4, col = \hlstr{"green"}, lty = \hlstr{"dotdash"})
\hlkwd{legend}(\hlstr{"right"}, , \hlkwd{c}(\hlstr{"OLS"}, \hlstr{"huber"}, \hlstr{"hampel"}, \hlstr{"bisquare"}), lty = \hlkwd{c}(\hlstr{"solid"}, 
    \hlstr{"dotted"}, \hlstr{"dashed"}, \hlstr{"dotdash"}), col = \hlkwd{c}(\hlstr{"black"}, \hlstr{"red"}, \hlstr{"blue"}, \hlstr{"green"}))
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-9} 

\end{knitrout}


从上图可以看出，由于异常点的出现，最小二乘法显然为照顾异常点，与预期有所偏离。而稳健回归却很好地削弱了异常点的影响。

model\_1最小二乘法得到的回归方程的系数和残差图：

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}(model_1)
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -75.28 -29.82 -15.17  -3.89 135.07 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept)    78.72      43.83    1.80     0.11
## x               2.01       1.38    1.46     0.18
## 
## Residual standard error: 64.9 on 8 degrees of freedom
## Multiple R-squared: 0.211,	Adjusted R-squared: 0.112 
## F-statistic: 2.14 on 1 and 8 DF,  p-value: 0.182
\end{verbatim}
\begin{alltt}
\hlkwd{plot}(\hlkwd{residuals}(model_1))
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-10} 

\end{knitrout}


model\_2为采用huber函数的稳健回归得到的回归方程的系数和残差图：

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}(model_2)
\end{alltt}
\begin{verbatim}
## 
## Call: rlm(formula = y ~ x, psi = psi.huber)
## Residuals:
##      Min       1Q   Median       3Q      Max 
##  -3.8106  -0.5529  -0.0887   0.7188 175.5865 
## 
## Coefficients:
##             Value  Std. Error t value
## (Intercept) 10.572  1.498      7.059 
## x            3.384  0.047     71.997 
## 
## Residual standard error: 1.14 on 8 degrees of freedom
\end{verbatim}
\begin{alltt}
\hlkwd{plot}(\hlkwd{residuals}(model_2))
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-11} 

\end{knitrout}


model\_3为采用Hampel三段截尾函数的稳健回归得到的回归方程的系数和残差图:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}(model_3)
\end{alltt}
\begin{verbatim}
## 
## Call: rlm(formula = y ~ x, psi = psi.hampel)
## Residuals:
##      Min       1Q   Median       3Q      Max 
##  -1.0754  -0.6476   0.0636   1.6577 176.9751 
## 
## Coefficients:
##             Value   Std. Error t value
## (Intercept)   7.926   0.895      8.852
## x             3.447   0.028    122.631
## 
## Residual standard error: 1.55 on 8 degrees of freedom
\end{verbatim}
\begin{alltt}
\hlkwd{plot}(\hlkwd{residuals}(model_3))
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-12} 

\end{knitrout}


model\_4为采用Tukey的双二次函数的稳健回归得到的回归方程的系数和残差图:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}(model_4)
\end{alltt}
\begin{verbatim}
## 
## Call: rlm(formula = y ~ x, psi = psi.bisquare)
## Residuals:
##      Min       1Q   Median       3Q      Max 
##  -1.0445  -0.6221   0.0902   1.6892 177.0090 
## 
## Coefficients:
##             Value   Std. Error t value
## (Intercept)   7.885   0.938      8.405
## x             3.447   0.029    117.057
## 
## Residual standard error: 1.5 on 8 degrees of freedom
\end{verbatim}
\begin{alltt}
\hlkwd{plot}(\hlkwd{residuals}(model_4))
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-13} 

\end{knitrout}



\section{总计}

从上面的分析我们可以看出，由于异常点的存在，最小二乘法在处理这些数据时，会受到不同程度的影响。而进行文件回归时，我们不需检测异常点，便可得到比较理想的结果。因此，在实际数据中，根据情况适当采用稳健回归是很有必要的。
\begin{thebibliography}{1}
\bibitem{key-1}Huber, P. J. (2011). Robust statistics (pp. 1248-1251).
Springer Berlin Heidelberg.

\bibitem{key-2}Fox, J. (2002). An R and S-Plus companion to applied
regression. Sage.

\bibitem{key-3}王谷, \& 过秀成. (2011). 包含异常数据的居民出行稳健回归分析. 武汉理工大学学报 (交通科学与工程版),
3, 019.

\bibitem{key-4}孙士兵, 赵欢, \& 贺宗梅. (2008). 基于稳健回归技术的软件成本估计方法. 科学技术与工程,
8(17), 4864-4868.

\bibitem{key-5}郭亚帆, \& 杜金柱. (2010). 经典回归与稳健回归方法的应用比较研究. 市场研究, (009),
17-21.\end{thebibliography}

\end{document}
